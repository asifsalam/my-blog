<h2>The multifaceted landscape of Artificial Intelligence</h2>

Symbolic AI Symbolic AI uses explicit rules, logic, and symbolic structures to replicate human
reasoning and knowledge representation. Main techniques Logic Programming: Encodes knowledge and
inferences using formal logic (e.g., Prolog). Expert Systems: Rule-based systems that mimic
domain-specific human expertise. Ontologies: Organizes knowledge hierarchically with defined
relationships (e.g., taxonomies). Semantic Networks: Represents knowledge as nodes (concepts) and
edges (relationships). Production Systems: Uses "if-then" rules to derive actions from conditions
(e.g., CLIPS). Main Algorithms A* Search: An optimization algorithm for graph traversal, using
heuristics to guide the search Depth-first / best-first search: Algorithms for systematically
traversing or searching through tree or graph data structures, each employing a distinct strategy to
explore nodes Backward Chaining: Goal-driven reasoning that traces backward through logical rules
Forward Chaining: Data-driven reasoning that applies rules to derive conclusions Resolution Theorem
Proving: Deduces new clauses by resolving contradictions in logic Constraint satisfaction
algorithms: Problem-solving methods that look for solutions meeting all specified constraints within
a given problem space Minimax Algorithm: Decision-making for adversarial games by minimizing loss
and maximizing gain. Example applications Expert systems: AI programs that use rule-based reasoning
to simulate human decision-making in specialized domains Symbolic mathematics: The use of AI
techniques to manipulate mathematical expressions symbolically rather than numerically Ontologies:
Structured representations of knowledge that define relationships between concepts in a domain
Automated theorem provers: AI systems that use formal logic to automatically prove mathematical and
logical theorems Semantic web: An extension of the World Wide Web that enables machines to interpret
and process data with semantic meaning Knowledge graphs: Graph-based data structures that represent
entities and their relationships to enable reasoning and inference Reasoning systems: AI frameworks
designed to infer new knowledge and make logical deductions based on given facts

<h3>Connectionist AI</h3>
Connectionist AI uses artificial neural networks to learn patterns from data, using inspiration from
our understanding of how the human brain works. Main techniques Artificial Neural Networks (ANNs): Interconnected
layers of nodes that learn hierarchical features Deep Learning: Neural networks with multiple layers,
typically with non-linear activation functions Convolutional Neural Networks (CNNs): Processes grid-like
data (e.g., images) with convolutional layers Recurrent Neural Networks (RNNs): Handles sequential data
via feedback loops Transformers: Uses self-attention mechanisms for parallelized sequence processing
Supervised unsupervised learning: Using labeled data to train models Unsupervised Learning: Discover
hidden patterns or structures in unlabeled data Example Algorithms Backpropagation: Adjusts network weights
by propagating errors backward Stochastic Gradient Descent (SGD): Optimizes models by iteratively updating
parameters Dropout: Regularization technique to prevent overfitting by randomly deactivating neurons
Adam Optimizer: Adaptive learning-rate algorithm combining RMSprop and momentum Attention Mechanism:
Enables models to focus on relevant parts of the input Example applications Text generation - AI programs
that generate natural language text based on input prompts. Image generation - AI programs that generate
images based on input prompts. Video generation- AI programs that generate videos from text descriptions.
Video to text - AI programs that transcribe videos into text. Reasoning models - Able to mimic thinking,
reasoning through complex tasks and solve harder problems. Image classification - Assigning one or more
predefined labels to an image based on its visual content. Text classification - AI programs that classify
text into predefined categories. Object detection - Identifying and localizing objects within images
or video frames. Speech recognition - AI programs that convert spoken language into written text. Anomaly
detection - Identifying patterns in data that deviate significantly from expected behavior. Forecasting
- Predicting future outcomes based on historical data. Recommendation systems - Predicting user preferences
to suggest relevant items or content.

<h3>Everything in between & outside</h3>
Other components of the AI Toolbox. Things that are part of machine learning and statistics, but do not
fit into either the Symbolic AI or Connectionist AI categories. There is a lot of overlap in the tasks,
but the techniques, algorithms and scale are different. Main Techniques Probability Theory: The broad
foundational mathematical tools used in machine learning to model uncertainty and analyze data distributions.
Statistical Inference: Using sample data to estimate population parameters and make predictions. Markov
Models: Models that assume the future state depends only on the current state, commonly used for sequential
data analysis. Bayesian Networks: Graphical models that represent probabilistic relationships among variables,
enabling inference under uncertainty. Evolutionary Algorithms: Optimization techniques inspired by natural
selection, iteratively evolving solutions to improve performance on a given task. Reinforcement Learning:
A learning paradigm where an agent interacts with an environment, receiving rewards or penalties to guide
its actions. Supervised Learning: A training approach using labeled data to teach models how to predict
outputs for new, unseen inputs. Unsupervised Learning: A training approach that discovers hidden patterns
or structures in unlabeled data without predefined output labels. Example algorithms Q-Learning: A way
for a computer to learn the best actions through trial and error, without needing a detailed model of
its surroundings. Policy Gradient Methods: Approaches that teach a computer to pick actions by directly
adjusting how it decides, aiming to maximize rewards. Expectation Maximization Algorithm: A step-by-step
process to refine guesses about hidden parts of a problem so they best match the data. Belief Propagation:
A method for updating how likely different possibilities are in a network of related ideas or events.
Hidden Markov Model: A way to describe something that changes over time, even when parts of it can't
be directly seen. Hypothesis Test: A procedure that helps decide if observed data strongly supports or
contradicts an initial idea. Genetic Programming: A technique that uses “survival of the fittest” to
automatically evolve computer programs to solve problems. K-means Clustering: A method that groups data
points into a chosen number of clusters by finding the best “centers” for each group. Hierarchical Clustering:
A way to organize data into groups step by step, either by combining smaller groups or splitting bigger
ones. Principal Component Analysis: A technique for identifying the main patterns in data, allowing you
to keep only the most important information. Linear Regression: A straightforward way to predict a number
(like a price) based on how it changes with other factors. Logistic Regression: A method to predict yes-or-no
outcomes by calculating how likely something is, given different inputs. Support Vector Machines: A tool
that searches for the best boundary to separate data into distinct categories. Random Forest: A collection
of decision trees that work together, combining their guesses for more accurate results. Traditional
ML tasks Image classification - assigning one or more predefined labels to an image based on its visual
content. Natural language processing - analyzing and understanding human language data to extract meaningful
information. Machine translation - converting text from one language into another. Sentiment analysis
- Determining the emotional tone or subjective opinion expressed in text. Object detection - identifying
and localizing objects within images or video frames. Text classification - AI programs that classify
text into predefined categories. Churn prediction - predicting which customers are likely to discontinue
using a service or product. Anomaly detection - identifying patterns in data that deviate significantly
from expected behavior. Recommendation systems - predicting user preferences to suggest relevant items
or content. Other examples, with other techniques Game Playing: Reinforcement learning algorithms like
Q-learning and policy gradients train AI agents to master board games, video games, and multi-agent environments.
Robotics: Reinforcement learning and evolutionary algorithms enable robots to learn tasks (e.g., navigation
or object manipulation) through trial-and-error interactions. Feature Reduction: Probability and statistics
methods (e.g., principal component analysis) condense high-dimensional data into more manageable, informative
features. Speech Recognition: Hidden Markov Models and Bayesian networks model and decode acoustic signals
to transform spoken language into text. Medical Diagnosis: Bayesian networks integrate uncertain factors
to support disease diagnosis and predict patient outcomes. Anomaly Detection: Statistical hypothesis
testing identifies unusual events or behaviors in large datasets, such as fraud detection. Genetic Programming
for Circuit Design: Evolving programs can search for optimal or near-optimal electronic circuit configurations.
Self-Driving Vehicles: Reinforcement learning helps autonomous cars learn optimal driving policies from
simulated or real-world experience. Recommendation Systems: Markov decision processes and Bayesian methods
adaptively recommend personalized content to users. Natural Language Processing: Expectation-maximization
algorithms underlie unsupervised models that handle tasks like machine translation or word alignment.
Please format this text with approporiate HTML tags to make it look good
